---
title: "TS Assignment 2"
author: "Ann Eitrheim"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading the data

```{r}
library('tseries')
df = read.csv('~/Documents/Time_Series_Analysis/Assignment 2/Assign 2 TS regression-1.csv')
nrows <- length(df[,1])
options( warn = -1 )
```

- **ISE**: Istanbul stock exchange national 100 index
- **SP**: Standard & Poorâ„¢s 500 return index
- **DAX**: Stock market return index of Germany
- **FTSE**: Stock market return index of UK
- **NIKKEI**: Stock market return index of Japan
- **BOVESPA**: Stock market return index of Brazil

***

# Questions

### 1.a. Determine if all the TS are stationary **qualitatively**

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot.ts(df[,2],main="ISE Time-Series Plot")
acf(df[,2],nrows,main="ACF Plot")
```

ISE is stationary because the mean and autocovariance are constant over time.

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot.ts(df[,3],main="SP Time-Series Plot")
acf(df[,3],nrows,main="ACF Plot")
```

SP is stationary because the mean and autocovariance are constant over time.

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot.ts(df[,4],main="DAX Time-Series Plot")
acf(df[,4],nrows,main="ACF Plot")
```

DAX is stationary because the mean and autocovariance are constant over time.

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot.ts(df[,5],main="FTSE Time-Series Plot")
acf(df[,5],nrows,main="ACF Plot")
```

FTSE is stationary because the mean and autocovariance are constant over time.

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot.ts(df[,6],main="NIKKEI Time-Series Plot")
acf(df[,6],nrows,main="ACF Plot")
```

NIKKEI is stationary because the mean and autocovariance are constant over time.

```{r, echo=FALSE}
par(mfrow=c(1,2))
plot.ts(df[,7],main="BOVESPA Time-Series Plot")
acf(df[,7],nrows,main="ACF Plot")
```

BOVESPA is stationary because the mean and autocovariance are constant over time.

### 1.b. Determine if all the TS are stationary **quantitatively**  - use ADF and KPSS from package tseries

```{r}
adf.test(df[,2]) #p-value = 0.01, reject Ho:non-stationary TS
kpss.test(df[,2]) #p-value = 0.1, do not reject Ho:stationary TS
```

ISE is stationary.

```{r}
adf.test(df[,3]) #p-value = 0.01, reject Ho:non-stationary TS
kpss.test(df[,3]) #p-value = 0.1, do not reject Ho:stationary TS
```

SP is stationary.

```{r}
adf.test(df[,4]) #p-value = 0.01, reject Ho:non-stationary TS
kpss.test(df[,4]) #p-value = 0.1, do not reject Ho:stationary TS
```

DAX is stationary.

```{r}
adf.test(df[,5]) #p-value = 0.01, reject Ho:non-stationary TS
kpss.test(df[,5]) #p-value = 0.1, do not reject Ho:stationary TS
```

FTSE is stationary.

```{r}
adf.test(df[,6]) #p-value = 0.01, reject Ho:non-stationary TS
kpss.test(df[,6]) #p-value = 0.1, do not reject Ho:stationary TS
```

NIKKEI is stationary.

```{r}
adf.test(df[,7]) #p-value = 0.01, reject Ho:non-stationary TS
kpss.test(df[,7]) #p-value = 0.1, do not reject Ho:stationary TS
```

BOVESPA is stationary.

### 2. Split the data into train and test, keeping only the last 10 rows for test (from date 9-Feb-11). Remember to use only train dataset for #3 to #6.

```{r}
train <- df[1:(nrows-10),]
test <- df[(nrows-9):nrows,]
```

### 3. Linearly regress ISE against the remaining 5 stock index returns - determine which coefficients are equal or better than 0.02 (*) level of significance?

```{r}
data=train[,2:7]
linMod <- lm(ISE ~ ., data=data)
summary(linMod)$coefficients[,4]<0.02
```

The cofficients of DAX, FTSE, and NIKKEI have P-values that are equal or better than 2%.
The coefficients of SP and BOVESPA do not have coefficients that are significant at that level.

### 4. For the non-significant coefficients, continue to lag by 1 day until all coefficients are significant at 0.01 (*). Use slide() function from package DataCombine. Remember you will need to lag, so you slideBy = -1 each step. How many lags are needed for each independent variable?

```{r}
library("DataCombine")
newdata <-slide(data, Var = 'BOVESPA', NewVar = 'BOVESPAlag1', slideBy = -1, reminder = F)
newdata <- na.omit(newdata)
newdata <- newdata[,c(1:5,7)]
linMod <- lm(ISE ~ ., data=newdata)
summary(linMod)$coefficients[,4]<0.01
```

BOVESPA is significant with lag = -1

```{r}
newdata <-slide(newdata, Var = 'SP', NewVar = 'SPlag1', slideBy = -1, reminder = F)
newdata <- na.omit(newdata)
newdata <- newdata[,c(1,3:7)]
linMod <- lm(ISE ~ ., data=newdata)
summary(linMod)
```

SPlag1 is not significant; pvalue = 0.311356 

```{r}
newdata <-slide(newdata, Var = 'SPlag1', NewVar = 'SPlag2', slideBy = -1, reminder = F)
newdata <- na.omit(newdata)
newdata <- newdata[,c(1:5,7)]
linMod <- lm(ISE ~ ., data=newdata)
summary(linMod)
summary(linMod)$coefficients[,4]<0.02
```

SP is now significant with lag = -2; pvalue = 0.01734
We need to lag BOVESPA by 1 and SP by 2 days.

### 5. Find correlations between ISE and each independent variable. Sum the square of the correlations. How does it compare to R-squared from #4?

```{r}
summary(linMod)$r.squared
sum(cor(newdata)[2:6,1]^2)
```

The number is ~twice as large.

### 6. Concept question 1 - why do you think the R-squared in #4 is so much less than the sum of square of the correlations?

This could be due to multicolinearity. The variables are correlated with one another; they are not  independent of each other. That is reasonable since market returns vary with each other. With a strong market, there would be positive returns expected from the indexes. A rising tide lifts all boats.

### 7. Take the test dataset - perform the same lags from #4 and call predict() function using the lm regression object from #4. Why do you need to use the lm function object from #4?

```{r}
data=test[,2:7]
newtestdata <-slide(data, Var = 'BOVESPA', NewVar = 'BOVESPAlag1', slideBy = -1, reminder = F)
newtestdata <-slide(newtestdata, Var = 'SP', NewVar = 'SPlag2', slideBy = -2, reminder = F)
newtestdata <- newtestdata[,c(1,3:5,7:8)]
newtestdata <- na.omit(newtestdata)
testPred <- predict(linMod, newtestdata)
mean(abs(data[3:10,1] - testPred))
```

The estimates are off by .034 on average.We used the lm object from #4 because it has the coefficients that apply to the lagged stock return indexes. It would not make sense to use the model on unlagged variables. Imagine using a model using November sales data to predict December sales but then giving the model sales numbers form September.

Due to lagging the variables, we made the first two rows of the test data have NAs. If we lagged the variables before splitting the train/test set, we could get all predictions. I did this just in case:

```{r}
data=df[,2:7]
newtestdata <-slide(data, Var = 'BOVESPA', NewVar = 'BOVESPAlag1', slideBy = -1, reminder = F)
newtestdata <-slide(newtestdata, Var = 'SP', NewVar = 'SPlag2', slideBy = -2, reminder = F)
newtestdata <- newtestdata[,c(1,3:5,7:8)]
newtestdata <- na.omit(newtestdata)
testPred <- predict(linMod, newtestdata)
testPred <- tail(testPred,10)
mean(abs(tail(data[,1],10) - testPred))
```

The estimates are off by .006 on average when using the part of the lagged data from the train set so there wouldn't be NAs.

### 8. Concept question 2 - what do you find in #1 and why?

Visually, the plot.ts graphs show that the means are stationary and the acf graphs show that the autocorrelation is constant over time. Additionally, the time series are shown to be stationary using the Augmented Dickey-Fuller (ADF) test and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. This means of the returns do not have an increasing or decreasing trend through time nor do they have varying autocovariance between set periods of time.